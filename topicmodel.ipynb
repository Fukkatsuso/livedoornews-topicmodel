{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topicmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPsGlyy8m2Ki3/xkB3N4vPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fukkatsuso/livedoornews-topicmodel/blob/master/topicmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-FlpHjkGkp",
        "colab_type": "text"
      },
      "source": [
        "# トピックモデル\n",
        "\n",
        "## Goal\n",
        "- ライブドアコーパスでWeb記事分類器を作る\n",
        "\n",
        "## Step\n",
        "1. ライブドアコーパスをスクレイピング\n",
        "1. データの前処理\n",
        "  1. 形態素解析 => **MeCab** (+NEologd)\n",
        "  1. 不要語の削除, 語の統一(ステミング)\n",
        "1. トピックモデルの構築 => **gensim**\n",
        "1. 機械学習 => **sklearn**\n",
        "\n",
        "## 参考\n",
        "- [LDAによるトピックモデル with gensim ~ Qiitaのタグからユーザーの嗜好を考える ~](https://qiita.com/shizuma/items/44c016812552ba8a8b88)\n",
        "- [トピックモデルをザックリと理解してサクッと試した](https://qiita.com/d-ogawa/items/c423cd4b01c6ed84a5e7)\n",
        "- [WordCloudとpyLDAvisによるLDAの可視化について](http://www.ie110704.net/2018/12/29/wordcloud%E3%81%A8pyldavis%E3%81%AB%E3%82%88%E3%82%8Blda%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/)\n",
        "- [自然言語処理による文書分類の基礎の基礎、トピックモデルを学ぶ](https://qiita.com/icoxfog417/items/7c944cb29dd7cdf5e2b1)\n",
        "- [scikit-learnとgensimでニュース記事を分類する](https://qiita.com/yasunori/items/31a23eb259482e4824e2)\n",
        "- [文書分類で自然言語処理に触れる](https://colab.research.google.com/drive/1IMjc-RTesapfNCEh0TPmg_ce_qAcV95b#scrollTo=9a9CUjgUXgB6)\n",
        "- [自然言語処理における前処理の種類とその威力](https://qiita.com/Hironsan/items/2466fe0f344115aff177)\n",
        "- [Python3×日本語：自然言語処理の前処理まとめ](https://qiita.com/chamao/items/7edaba62b120a660657e)\n",
        "- [ニュース記事の分類を機械学習で予測する](https://qiita.com/hyo_07/items/ba3d53868b2f55ed9941)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BalvotaRLXB0",
        "colab_type": "text"
      },
      "source": [
        "## データ収集\n",
        "### 対象\n",
        "- [livedoorニュースコーパス](https://www.rondhuit.com/download.html#ldcc)\n",
        "  - [トピックニュース](http://news.livedoor.com/category/vender/news/)\n",
        "  - [Sports Watch](http://news.livedoor.com/category/vender/208/)\n",
        "  - [ITライフハック](http://news.livedoor.com/category/vender/223/)\n",
        "  - [家電チャンネル](http://news.livedoor.com/category/vender/kadench/)\n",
        "  - [MOVIE ENTER](http://news.livedoor.com/category/vender/movie_enter/)\n",
        "  - [独女通信](http://news.livedoor.com/category/vender/90/)\n",
        "  - [エスマックス](http://news.livedoor.com/category/vender/smax/)\n",
        "  - [livedoor HOMME](http://news.livedoor.com/category/vender/homme/)\n",
        "  - [Peachy](http://news.livedoor.com/category/vender/ldgirls/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ0Fu1bAV2G3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dataset\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!mkdir -p dataset/livedoor && tar xvzf ldcc-20140209.tar.gz -C /content/dataset/livedoor --strip-components 1\n",
        "!rm ldcc-20140209.tar.gz\n",
        "\n",
        "# Install MeCab\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "\n",
        "# Install mecab-ipadic-NEologd\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n",
        "\n",
        "!pip install mojimoji"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlMKbg72yh6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 確認\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQF6coxgmfPW",
        "colab_type": "text"
      },
      "source": [
        "## 1.前処理なしLDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhdLIs1SElQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import glob2\n",
        "import MeCab\n",
        "\n",
        "mecab = MeCab.Tagger(\"-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
        "\n",
        "paths = glob2.glob(\"dataset/livedoor/sports-watch/*-*.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPAsBvLueQiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "for path in paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  title = data[2]\n",
        "  words.append(mecab.parse(title).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMnvyMC5eaKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 辞書, コーパス作成\n",
        "dictionary = gensim.corpora.Dictionary(words)\n",
        "\n",
        "dictionary.save_as_text(\"dictionary1.dict.txt\")\n",
        "\n",
        "corpus = [dictionary.doc2bow(w) for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goc0abc8LrSV",
        "colab_type": "code",
        "outputId": "3a515e49-98a7-49c0-83de-534ebb7dfd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# LDA\n",
        "topic_N = 10\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=topic_N, id2word=dictionary)\n",
        "\n",
        "for i in range(topic_N):\n",
        "  print('TOPIC:', i, '=>', lda.print_topic(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 900 documents, updating model once every 900 documents, evaluating perplexity every 900 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "-11.454 per-word bound, 2805.7 perplexity estimate based on a held-out corpus of 900 documents with 14123 words\n",
            "PROGRESS: pass 0, at document #900/900\n",
            "topic #0 (0.100): 0.053*\"の\" + 0.039*\"】\" + 0.039*\"に\" + 0.039*\"【\" + 0.038*\"Watch\" + 0.037*\"Sports\" + 0.033*\"「\" + 0.032*\"」\" + 0.030*\"、\" + 0.023*\"が\"\n",
            "topic #4 (0.100): 0.036*\"「\" + 0.036*\"」\" + 0.032*\"に\" + 0.031*\"が\" + 0.024*\"は\" + 0.021*\"の\" + 0.017*\"・\" + 0.016*\"Sports\" + 0.016*\"【\" + 0.016*\"Watch\"\n",
            "topic #2 (0.100): 0.036*\"、\" + 0.030*\"Sports\" + 0.030*\"【\" + 0.030*\"Watch\" + 0.029*\"】\" + 0.027*\"「\" + 0.026*\"」\" + 0.022*\"に\" + 0.019*\"を\" + 0.014*\"た\"\n",
            "topic #8 (0.100): 0.054*\"の\" + 0.026*\"に\" + 0.025*\"、\" + 0.020*\"は\" + 0.020*\"た\" + 0.018*\"Sports\" + 0.018*\"【\" + 0.017*\"Watch\" + 0.017*\"】\" + 0.017*\"を\"\n",
            "topic #6 (0.100): 0.030*\"・\" + 0.028*\"の\" + 0.028*\"は\" + 0.021*\"Sports\" + 0.021*\"、\" + 0.021*\"【\" + 0.020*\"】\" + 0.020*\"Watch\" + 0.020*\"に\" + 0.012*\"と\"\n",
            "topic diff=7.406228, rho=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TOPIC: 0 => 0.053*\"の\" + 0.039*\"】\" + 0.039*\"に\" + 0.039*\"【\" + 0.038*\"Watch\" + 0.037*\"Sports\" + 0.033*\"「\" + 0.032*\"」\" + 0.030*\"、\" + 0.023*\"が\"\n",
            "TOPIC: 1 => 0.022*\"に\" + 0.009*\"Sports\" + 0.009*\"Watch\" + 0.009*\"【\" + 0.009*\"た\" + 0.009*\"、\" + 0.008*\"】\" + 0.007*\"”\" + 0.007*\"“\" + 0.007*\"ない\"\n",
            "TOPIC: 2 => 0.036*\"、\" + 0.030*\"Sports\" + 0.030*\"【\" + 0.030*\"Watch\" + 0.029*\"】\" + 0.027*\"「\" + 0.026*\"」\" + 0.022*\"に\" + 0.019*\"を\" + 0.014*\"た\"\n",
            "TOPIC: 3 => 0.035*\"の\" + 0.027*\"」\" + 0.026*\"「\" + 0.025*\"は\" + 0.025*\"Sports\" + 0.024*\"【\" + 0.024*\"Watch\" + 0.024*\"】\" + 0.021*\"、\" + 0.021*\"た\"\n",
            "TOPIC: 4 => 0.036*\"「\" + 0.036*\"」\" + 0.032*\"に\" + 0.031*\"が\" + 0.024*\"は\" + 0.021*\"の\" + 0.017*\"・\" + 0.016*\"Sports\" + 0.016*\"【\" + 0.016*\"Watch\"\n",
            "TOPIC: 5 => 0.040*\"】\" + 0.040*\"、\" + 0.039*\"Sports\" + 0.039*\"Watch\" + 0.039*\"【\" + 0.038*\"の\" + 0.023*\"は\" + 0.022*\"に\" + 0.020*\"「\" + 0.020*\"」\"\n",
            "TOPIC: 6 => 0.030*\"・\" + 0.028*\"の\" + 0.028*\"は\" + 0.021*\"Sports\" + 0.021*\"、\" + 0.021*\"【\" + 0.020*\"】\" + 0.020*\"Watch\" + 0.020*\"に\" + 0.012*\"と\"\n",
            "TOPIC: 7 => 0.047*\"、\" + 0.039*\"Sports\" + 0.039*\"】\" + 0.038*\"Watch\" + 0.038*\"【\" + 0.034*\"の\" + 0.030*\"は\" + 0.018*\"「\" + 0.018*\"」\" + 0.016*\"に\"\n",
            "TOPIC: 8 => 0.054*\"の\" + 0.026*\"に\" + 0.025*\"、\" + 0.020*\"は\" + 0.020*\"た\" + 0.018*\"Sports\" + 0.018*\"【\" + 0.017*\"Watch\" + 0.017*\"】\" + 0.017*\"を\"\n",
            "TOPIC: 9 => 0.046*\"の\" + 0.040*\"に\" + 0.036*\"、\" + 0.029*\"」\" + 0.029*\"「\" + 0.023*\"が\" + 0.018*\"は\" + 0.016*\"“\" + 0.016*\"”\" + 0.015*\"Watch\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfuiKA1Zdzsu",
        "colab_type": "text"
      },
      "source": [
        "## 2.前処理ありLDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsVSdhHrlXPe",
        "colab_type": "text"
      },
      "source": [
        "1. 正規化\n",
        "  - 半角かな => 全角かな\n",
        "  - 全角英数 => 半角英数\n",
        "  - 大文字 => 小文字\n",
        "  - 辞書による統一?\n",
        "1. 品詞で取捨選択\n",
        "1. ストップワード除去\n",
        "  - 辞書\n",
        "    - [SlothLib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXl8SSGsfy1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import glob2\n",
        "import mojimoji\n",
        "import nltk\n",
        "import MeCab\n",
        "import re\n",
        "import urllib3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "mecab = MeCab.Tagger(\"mecabrc -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
        "\n",
        "paths = glob2.glob(\"dataset/livedoor/*/*-*.txt\")\n",
        "\n",
        "# 学習用と評価用に分ける\n",
        "train_rate = 0.7\n",
        "random_state = 0\n",
        "train_article_paths, test_article_paths = train_test_split(paths, train_size=train_rate, random_state=random_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X7IZ8VuqxCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# カテゴリごとの記事数の分散\n",
        "def deviation(random_state, article_paths):\n",
        "  cat = {}\n",
        "  for path in article_paths:\n",
        "    c = path.split('/')[2]\n",
        "    if cat.get(c) == None:\n",
        "      cat[c] = 0\n",
        "    else:\n",
        "      cat[c] += 1\n",
        "\n",
        "  avr = 0\n",
        "  for c in cat:\n",
        "    avr += cat[c]\n",
        "  avr /= len(cat)\n",
        "\n",
        "  d = 0\n",
        "  for c in cat:\n",
        "    d += (cat[c]-avr)**2\n",
        "\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWZB9OOuk7LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ストップワードのリスト\n",
        "def get_stopwords():\n",
        "  # SlothLib\n",
        "  slothlib_url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "  http = urllib3.PoolManager()\n",
        "  res = http.request('GET', slothlib_url)\n",
        "  stopwords = res.data.decode('utf-8').split()\n",
        "  # nltk\n",
        "  stopwords.extend(nltk.corpus.stopwords.words(\"english\"))\n",
        "  # 自分で設定\n",
        "  swlist = ['さ', 'の', 'ら', 'ん', '0']\n",
        "  stopwords.extend(swlist)\n",
        "  return stopwords\n",
        "\n",
        "stopwords = get_stopwords()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YEqeIS-heR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 単語のリストから品詞を限定して取り出す\n",
        "def extract_by_parts(parsed, parts):\n",
        "  words = []\n",
        "  lines = parsed.split('\\n')\n",
        "  for line in lines:\n",
        "    feature = line.split('\\t')\n",
        "    if len(feature) < 2:\n",
        "      continue\n",
        "    info = feature[1].split(',')\n",
        "    if info[0] in parts:\n",
        "      if info[6] == '*': \n",
        "        words.append(feature[0])  # 活用なしの語\n",
        "      else:  \n",
        "        words.append(info[6]) # 表記ゆれの対処\n",
        "  return words\n",
        "\n",
        "# 各単語を前処理にかける\n",
        "def preprocess_words(words):\n",
        "  for i in range(len(words)):\n",
        "    words[i] = unify_chartype(words[i])\n",
        "  words = filter_stopwords(words)\n",
        "  return words\n",
        "\n",
        "# 文字種を統一する\n",
        "def unify_chartype(text):\n",
        "  text = mojimoji.zen_to_han(text, kana=False, digit=True, ascii=True) # 全角英数=>半角英数\n",
        "  text = mojimoji.han_to_zen(text, kana=True, digit=False, ascii=False) # 半角かな=>全角かな\n",
        "  text = text.lower() # 大文字=>小文字\n",
        "  text = re.sub('\\d+', '0', text)  # 数字列を0に置き換え\n",
        "  return text\n",
        "\n",
        "# ストップワードを除去する\n",
        "def filter_stopwords(words):\n",
        "  filtered_words = [word for word in words if word not in stopwords]\n",
        "  return filtered_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whEiJhybgqfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テキストを分解して品詞で絞り込み，各単語を前処理したリストを返す\n",
        "def text2words(text, parts):\n",
        "  parsed_text = mecab.parse(text)\n",
        "  words = extract_by_parts(parsed_text, parts)\n",
        "  words = preprocess_words(words)\n",
        "  return words\n",
        "\n",
        "# 取り出す品詞\n",
        "parts = ['名詞']\n",
        "\n",
        "# 記事タイトルから学習に必要な単語を取り出す\n",
        "train_words = []\n",
        "for path in train_article_paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  words = text2words(data[2], parts)\n",
        "  train_words.append(words)\n",
        "\n",
        "# テスト用データ\n",
        "test_words = []\n",
        "for path in test_article_paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  words = text2words(data[2], parts)\n",
        "  test_words.append(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkHuyZgcjmMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words2corpus(dictionary, words):\n",
        "  return [dictionary.doc2bow(w) for w in words]\n",
        "\n",
        "# 辞書作成\n",
        "dictionary = gensim.corpora.Dictionary(train_words)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.2)\n",
        "dictionary.save_as_text(\"dictionary2.dict.txt\")\n",
        "\n",
        "# コーパス作成\n",
        "train_corpus = words2corpus(dictionary, train_words)\n",
        "\n",
        "# テスト用データ\n",
        "test_corpus = words2corpus(dictionary, test_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjRFY-jETg0Q",
        "colab_type": "code",
        "outputId": "4d8c0aa2-cede-4bf2-9ba1-75d0d06cce58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# LDA\n",
        "topic_N = 20\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=train_corpus, num_topics=topic_N, id2word=dictionary)\n",
        "\n",
        "for i in range(topic_N):\n",
        "  print('TOPIC:', i, '=>', lda.print_topic(i))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOPIC: 0 => 0.059*\"話題\" + 0.056*\"0万円\" + 0.043*\"年収\" + 0.023*\"twitter\" + 0.019*\"快適\" + 0.017*\"女性\" + 0.016*\"volume.0\" + 0.016*\"大丈夫\" + 0.015*\"cm\" + 0.014*\"pc\"\n",
            "TOPIC: 1 => 0.095*\"チェック\" + 0.073*\"売れ筋\" + 0.024*\"ブランド\" + 0.019*\"cafe\" + 0.019*\"節電\" + 0.019*\"vol.0\" + 0.019*\"発売\" + 0.017*\"presented\" + 0.015*\"今度\" + 0.014*\"結婚\"\n",
            "TOPIC: 2 => 0.035*\"ゴルフ\" + 0.031*\"氏\" + 0.028*\"監督\" + 0.028*\"watch\" + 0.028*\"sports\" + 0.023*\"巨人\" + 0.019*\"韓国ニュース\" + 0.019*\"facebook\" + 0.016*\"復帰\" + 0.016*\"交際\"\n",
            "TOPIC: 3 => 0.047*\"部屋\" + 0.045*\"volume.0\" + 0.044*\"説教\" + 0.044*\"辛口\" + 0.016*\"対決\" + 0.015*\"シャープ\" + 0.014*\"チェック\" + 0.014*\"解消\" + 0.014*\"話題\" + 0.012*\"キーワード\"\n",
            "TOPIC: 4 => 0.039*\"写真\" + 0.028*\"レポート\" + 0.016*\"sc\" + 0.014*\"0位\" + 0.013*\"反応\" + 0.013*\"大島優子\" + 0.013*\"アナタ\" + 0.013*\"galaxy\" + 0.013*\"テクニック\" + 0.013*\"韓国\"\n",
            "TOPIC: 5 => 0.173*\"sports\" + 0.173*\"watch\" + 0.012*\"日本\" + 0.011*\"0年\" + 0.011*\"0d\" + 0.011*\"0月0日\" + 0.010*\"氏\" + 0.010*\"期待\" + 0.009*\"結婚\" + 0.009*\"話題\"\n",
            "TOPIC: 6 => 0.061*\"スマホ\" + 0.035*\"対応\" + 0.034*\"0月0日\" + 0.030*\"nttドコモ\" + 0.025*\"xi\" + 0.021*\"女子\" + 0.019*\"バッテリー\" + 0.019*\"iphone\" + 0.019*\"ドコモ\" + 0.015*\"f-0\"\n",
            "TOPIC: 7 => 0.029*\"アプリ\" + 0.025*\"登場\" + 0.021*\"サービス\" + 0.020*\"解説\" + 0.018*\"iphoneアプリ\" + 0.017*\"情報\" + 0.016*\"レビュー\" + 0.016*\"機種\" + 0.016*\"カメラ\" + 0.016*\"試写会\"\n",
            "TOPIC: 8 => 0.022*\"volume.0\" + 0.021*\"web\" + 0.021*\"tweet\" + 0.019*\"0万円\" + 0.017*\"年収\" + 0.017*\"ノート\" + 0.015*\"onna\" + 0.015*\"中国\" + 0.014*\"ビジネス\" + 0.013*\"図鑑\"\n",
            "TOPIC: 9 => 0.034*\"韓国\" + 0.030*\"話題\" + 0.023*\"今年\" + 0.021*\"選手\" + 0.020*\"注目\" + 0.020*\"紺子\" + 0.019*\"コスメ\" + 0.018*\"美女\" + 0.018*\"田中\" + 0.017*\"スタート\"\n",
            "TOPIC: 10 => 0.048*\"搭載\" + 0.035*\"向け\" + 0.030*\"発表\" + 0.025*\"smartphone\" + 0.023*\"android icecream sandwich\" + 0.023*\"au\" + 0.022*\"提供開始\" + 0.022*\"nttドコモ\" + 0.022*\"更新\" + 0.022*\"対応\"\n",
            "TOPIC: 11 => 0.078*\"映画\" + 0.034*\"オトナ女子\" + 0.030*\"0年\" + 0.020*\"特集\" + 0.018*\"リアル\" + 0.018*\"androidアプリ\" + 0.018*\"編\" + 0.018*\"公開\" + 0.017*\"オススメ\" + 0.015*\"悩み\"\n",
            "TOPIC: 12 => 0.030*\"活用\" + 0.029*\"androidアプリ\" + 0.025*\"インタビュー\" + 0.023*\"チェック\" + 0.022*\"術\" + 0.022*\"転職\" + 0.020*\"男性\" + 0.020*\"パソコン\" + 0.019*\"話題\" + 0.019*\"売れ筋\"\n",
            "TOPIC: 13 => 0.027*\"女優\" + 0.023*\"ソフトバンク\" + 0.022*\"話題\" + 0.019*\"劇場\" + 0.018*\"開発\" + 0.016*\"対応\" + 0.016*\"妖\" + 0.016*\"ヶ\" + 0.016*\"deji\" + 0.015*\"ssd\"\n",
            "TOPIC: 14 => 0.034*\"google\" + 0.031*\"声\" + 0.029*\"ロゴ\" + 0.027*\"批判\" + 0.025*\"発言\" + 0.019*\"話題\" + 0.017*\"共有\" + 0.015*\"第0弾\" + 0.015*\"先行\" + 0.015*\"akb0\"\n",
            "TOPIC: 15 => 0.088*\"話題\" + 0.024*\"ネット\" + 0.023*\"まとめ\" + 0.022*\"watch\" + 0.022*\"sports\" + 0.021*\"映画\" + 0.020*\"読み\" + 0.020*\"週末\" + 0.019*\"blog\" + 0.015*\"告白\"\n",
            "TOPIC: 16 => 0.040*\"決定\" + 0.039*\"プレゼント\" + 0.031*\"公開\" + 0.030*\"終了\" + 0.019*\"女性\" + 0.017*\"婚活\" + 0.016*\"paul smith\" + 0.015*\"寿司\" + 0.015*\"クルマ\" + 0.014*\"映画\"\n",
            "TOPIC: 17 => 0.043*\"iphone\" + 0.036*\"0人\" + 0.035*\"開催\" + 0.033*\"チャンス\" + 0.017*\"映像\" + 0.016*\"モバイル\" + 0.015*\"最高\" + 0.015*\"お気に入り\" + 0.015*\"国内\" + 0.014*\"風\"\n",
            "TOPIC: 18 => 0.076*\"独女\" + 0.033*\"仕事\" + 0.025*\"週間ランキング\" + 0.019*\"最新作\" + 0.019*\"好き\" + 0.017*\"0歳\" + 0.015*\"夢\" + 0.015*\"サポート\" + 0.014*\"恋愛\" + 0.014*\"0回\"\n",
            "TOPIC: 19 => 0.050*\"話題\" + 0.026*\"レポート\" + 0.024*\"日本\" + 0.023*\"動画\" + 0.023*\"紹介\" + 0.022*\"写真\" + 0.022*\"声\" + 0.019*\"ニュース\" + 0.016*\"nttドコモ\" + 0.015*\"軽量\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhB-fWKnoWMo",
        "colab_type": "text"
      },
      "source": [
        "## 機械学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6uB7O0jBI43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データ準備\n",
        "\n",
        "# カテゴリ\n",
        "category = {\n",
        "  'dokujo-tsushin': 0,\n",
        "  'it-life-hack': 1,\n",
        "  'kaden-channel': 2,\n",
        "  'livedoor-homme': 3,\n",
        "  'movie-enter': 4,\n",
        "  'peachy': 5,\n",
        "  'smax': 6,\n",
        "  'sports-watch': 7,\n",
        "  'topic-news': 8\n",
        "}\n",
        "\n",
        "def data_and_labels(paths, corpus, lda, topic_N):\n",
        "  data = []\n",
        "  labels = []\n",
        "  for i in range(len(paths)):\n",
        "    # data\n",
        "    w = [0] * topic_N\n",
        "    for topic in lda[corpus[i]]:\n",
        "      w[topic[0]] = topic[1]\n",
        "    data.append(w)\n",
        "    # label\n",
        "    label = category[paths[i].split('/')[2]]\n",
        "    labels.append(label)\n",
        "  return data, labels\n",
        "\n",
        "# トピックモデル構築に使ったデータ\n",
        "train_data, train_label = data_and_labels(train_article_paths, train_corpus, lda, topic_N)\n",
        "\n",
        "# テスト用データ\n",
        "test_data, test_label = data_and_labels(test_article_paths, test_corpus, lda, topic_N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqxbLPjbA87Z",
        "colab_type": "text"
      },
      "source": [
        "### ランダムフォレスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaweF0MmoaWS",
        "colab_type": "code",
        "outputId": "e66e2c2e-3f35-44f8-ba96-538fa9f58f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(train_data, train_label)\n",
        "\n",
        "# 精度評価\n",
        "random_forest.score(test_data, test_label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40298507462686567"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTzHvTxBsbP",
        "colab_type": "text"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cNac7QrB0gu",
        "colab_type": "code",
        "outputId": "473c92bb-7d23-47bf-9929-7b787d781643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf.fit(train_data, train_label)\n",
        "\n",
        "# 精度評価\n",
        "clf.score(test_data, test_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3939393939393939"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}