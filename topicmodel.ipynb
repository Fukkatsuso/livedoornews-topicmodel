{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topicmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOJL3XStAExkAsVYb7SqLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fukkatsuso/livedoornews-topicmodel/blob/master/topicmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-FlpHjkGkp",
        "colab_type": "text"
      },
      "source": [
        "# トピックモデル\n",
        "\n",
        "## Goal\n",
        "- ライブドアコーパスでWeb記事分類器を作る\n",
        "\n",
        "## Step\n",
        "1. ライブドアコーパスをスクレイピング\n",
        "1. データの前処理\n",
        "  1. 形態素解析 => **MeCab** (+NEologd)\n",
        "  1. 不要語の削除, 語の統一(ステミング)\n",
        "1. トピックモデル(特徴ベクトル)の構築 => **gensim**\n",
        "1. 機械学習 => **sklearn**\n",
        "\n",
        "## 参考\n",
        "- [LDAによるトピックモデル with gensim ~ Qiitaのタグからユーザーの嗜好を考える ~](https://qiita.com/shizuma/items/44c016812552ba8a8b88)\n",
        "- [トピックモデルをザックリと理解してサクッと試した](https://qiita.com/d-ogawa/items/c423cd4b01c6ed84a5e7)\n",
        "- [WordCloudとpyLDAvisによるLDAの可視化について](http://www.ie110704.net/2018/12/29/wordcloud%E3%81%A8pyldavis%E3%81%AB%E3%82%88%E3%82%8Blda%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/)\n",
        "- [自然言語処理による文書分類の基礎の基礎、トピックモデルを学ぶ](https://qiita.com/icoxfog417/items/7c944cb29dd7cdf5e2b1)\n",
        "- [scikit-learnとgensimでニュース記事を分類する](https://qiita.com/yasunori/items/31a23eb259482e4824e2)\n",
        "- [文書分類で自然言語処理に触れる](https://colab.research.google.com/drive/1IMjc-RTesapfNCEh0TPmg_ce_qAcV95b#scrollTo=9a9CUjgUXgB6)\n",
        "- [自然言語処理における前処理の種類とその威力](https://qiita.com/Hironsan/items/2466fe0f344115aff177)\n",
        "- [Python3×日本語：自然言語処理の前処理まとめ](https://qiita.com/chamao/items/7edaba62b120a660657e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BalvotaRLXB0",
        "colab_type": "text"
      },
      "source": [
        "## データ収集\n",
        "### 対象\n",
        "- [livedoorニュースコーパス](https://www.rondhuit.com/download.html#ldcc)\n",
        "  - [トピックニュース](http://news.livedoor.com/category/vender/news/)\n",
        "  - [Sports Watch](http://news.livedoor.com/category/vender/208/)\n",
        "  - [ITライフハック](http://news.livedoor.com/category/vender/223/)\n",
        "  - [家電チャンネル](http://news.livedoor.com/category/vender/kadench/)\n",
        "  - [MOVIE ENTER](http://news.livedoor.com/category/vender/movie_enter/)\n",
        "  - [独女通信](http://news.livedoor.com/category/vender/90/)\n",
        "  - [エスマックス](http://news.livedoor.com/category/vender/smax/)\n",
        "  - [livedoor HOMME](http://news.livedoor.com/category/vender/homme/)\n",
        "  - [Peachy](http://news.livedoor.com/category/vender/ldgirls/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ0Fu1bAV2G3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dataset\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!mkdir -p dataset/livedoor && tar xvzf ldcc-20140209.tar.gz -C /content/dataset/livedoor --strip-components 1\n",
        "!rm ldcc-20140209.tar.gz\n",
        "\n",
        "# Install MeCab\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "\n",
        "# Install mecab-ipadic-NEologd\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlMKbg72yh6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 確認\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQF6coxgmfPW",
        "colab_type": "text"
      },
      "source": [
        "## 1.前処理なしLDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhdLIs1SElQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import glob2\n",
        "import MeCab\n",
        "\n",
        "mecab = MeCab.Tagger(\"-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
        "\n",
        "paths = glob2.glob(\"dataset/livedoor/sports-watch/*-*.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPAsBvLueQiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "for path in paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  title = data[2]\n",
        "  words.append(mecab.parse(title).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMnvyMC5eaKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 辞書, コーパス作成\n",
        "dictionary = gensim.corpora.Dictionary(words)\n",
        "\n",
        "dictionary.save_as_text(\"dictionary1.dict.txt\")\n",
        "\n",
        "corpus = [dictionary.doc2bow(w) for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goc0abc8LrSV",
        "colab_type": "code",
        "outputId": "3a515e49-98a7-49c0-83de-534ebb7dfd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# LDA\n",
        "topic_N = 10\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=topic_N, id2word=dictionary)\n",
        "\n",
        "for i in range(topic_N):\n",
        "  print('TOPIC:', i, '=>', lda.print_topic(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 900 documents, updating model once every 900 documents, evaluating perplexity every 900 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "-11.454 per-word bound, 2805.7 perplexity estimate based on a held-out corpus of 900 documents with 14123 words\n",
            "PROGRESS: pass 0, at document #900/900\n",
            "topic #0 (0.100): 0.053*\"の\" + 0.039*\"】\" + 0.039*\"に\" + 0.039*\"【\" + 0.038*\"Watch\" + 0.037*\"Sports\" + 0.033*\"「\" + 0.032*\"」\" + 0.030*\"、\" + 0.023*\"が\"\n",
            "topic #4 (0.100): 0.036*\"「\" + 0.036*\"」\" + 0.032*\"に\" + 0.031*\"が\" + 0.024*\"は\" + 0.021*\"の\" + 0.017*\"・\" + 0.016*\"Sports\" + 0.016*\"【\" + 0.016*\"Watch\"\n",
            "topic #2 (0.100): 0.036*\"、\" + 0.030*\"Sports\" + 0.030*\"【\" + 0.030*\"Watch\" + 0.029*\"】\" + 0.027*\"「\" + 0.026*\"」\" + 0.022*\"に\" + 0.019*\"を\" + 0.014*\"た\"\n",
            "topic #8 (0.100): 0.054*\"の\" + 0.026*\"に\" + 0.025*\"、\" + 0.020*\"は\" + 0.020*\"た\" + 0.018*\"Sports\" + 0.018*\"【\" + 0.017*\"Watch\" + 0.017*\"】\" + 0.017*\"を\"\n",
            "topic #6 (0.100): 0.030*\"・\" + 0.028*\"の\" + 0.028*\"は\" + 0.021*\"Sports\" + 0.021*\"、\" + 0.021*\"【\" + 0.020*\"】\" + 0.020*\"Watch\" + 0.020*\"に\" + 0.012*\"と\"\n",
            "topic diff=7.406228, rho=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TOPIC: 0 => 0.053*\"の\" + 0.039*\"】\" + 0.039*\"に\" + 0.039*\"【\" + 0.038*\"Watch\" + 0.037*\"Sports\" + 0.033*\"「\" + 0.032*\"」\" + 0.030*\"、\" + 0.023*\"が\"\n",
            "TOPIC: 1 => 0.022*\"に\" + 0.009*\"Sports\" + 0.009*\"Watch\" + 0.009*\"【\" + 0.009*\"た\" + 0.009*\"、\" + 0.008*\"】\" + 0.007*\"”\" + 0.007*\"“\" + 0.007*\"ない\"\n",
            "TOPIC: 2 => 0.036*\"、\" + 0.030*\"Sports\" + 0.030*\"【\" + 0.030*\"Watch\" + 0.029*\"】\" + 0.027*\"「\" + 0.026*\"」\" + 0.022*\"に\" + 0.019*\"を\" + 0.014*\"た\"\n",
            "TOPIC: 3 => 0.035*\"の\" + 0.027*\"」\" + 0.026*\"「\" + 0.025*\"は\" + 0.025*\"Sports\" + 0.024*\"【\" + 0.024*\"Watch\" + 0.024*\"】\" + 0.021*\"、\" + 0.021*\"た\"\n",
            "TOPIC: 4 => 0.036*\"「\" + 0.036*\"」\" + 0.032*\"に\" + 0.031*\"が\" + 0.024*\"は\" + 0.021*\"の\" + 0.017*\"・\" + 0.016*\"Sports\" + 0.016*\"【\" + 0.016*\"Watch\"\n",
            "TOPIC: 5 => 0.040*\"】\" + 0.040*\"、\" + 0.039*\"Sports\" + 0.039*\"Watch\" + 0.039*\"【\" + 0.038*\"の\" + 0.023*\"は\" + 0.022*\"に\" + 0.020*\"「\" + 0.020*\"」\"\n",
            "TOPIC: 6 => 0.030*\"・\" + 0.028*\"の\" + 0.028*\"は\" + 0.021*\"Sports\" + 0.021*\"、\" + 0.021*\"【\" + 0.020*\"】\" + 0.020*\"Watch\" + 0.020*\"に\" + 0.012*\"と\"\n",
            "TOPIC: 7 => 0.047*\"、\" + 0.039*\"Sports\" + 0.039*\"】\" + 0.038*\"Watch\" + 0.038*\"【\" + 0.034*\"の\" + 0.030*\"は\" + 0.018*\"「\" + 0.018*\"」\" + 0.016*\"に\"\n",
            "TOPIC: 8 => 0.054*\"の\" + 0.026*\"に\" + 0.025*\"、\" + 0.020*\"は\" + 0.020*\"た\" + 0.018*\"Sports\" + 0.018*\"【\" + 0.017*\"Watch\" + 0.017*\"】\" + 0.017*\"を\"\n",
            "TOPIC: 9 => 0.046*\"の\" + 0.040*\"に\" + 0.036*\"、\" + 0.029*\"」\" + 0.029*\"「\" + 0.023*\"が\" + 0.018*\"は\" + 0.016*\"“\" + 0.016*\"”\" + 0.015*\"Watch\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfuiKA1Zdzsu",
        "colab_type": "text"
      },
      "source": [
        "## 2.前処理ありLDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsVSdhHrlXPe",
        "colab_type": "text"
      },
      "source": [
        "1. 正規化\n",
        "  - 半角かな => 全角かな\n",
        "  - 全角英数 => 半角英数\n",
        "  - 大文字 => 小文字\n",
        "  - 辞書による統一?\n",
        "1. 品詞で取捨選択\n",
        "1. ストップワード除去\n",
        "  - 辞書\n",
        "    - [SlothLib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFdHbRIpJAUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mojimoji"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXl8SSGsfy1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import glob2\n",
        "import mojimoji\n",
        "import MeCab\n",
        "import urllib3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mecab = MeCab.Tagger(\"mecabrc -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
        "\n",
        "paths = glob2.glob(\"dataset/livedoor/*/*-*.txt\")\n",
        "\n",
        "# 学習用と評価用に分ける\n",
        "train_rate = 0.8\n",
        "train_article_paths, test_article_paths = train_test_split(paths, train_size=train_rate, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YEqeIS-heR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# パースされた文字列から品詞を限定して取り出す\n",
        "def extract_by_parts(parsed, parts):\n",
        "  words = []\n",
        "  lines = parsed.split('\\n')\n",
        "  for line in lines:\n",
        "    feature = line.split('\\t')\n",
        "    if len(feature) == 2:\n",
        "      info = feature[1].split(',')\n",
        "      if info[0] in parts:\n",
        "        if info[6] == '*': \n",
        "          words.append(feature[0])  # 活用なしの語\n",
        "        else:  \n",
        "          words.append(info[6]) # 表記ゆれの対処\n",
        "  return words\n",
        "\n",
        "# 各単語を前処理にかける\n",
        "def preprocess_words(words, stopwords):\n",
        "  for i in range(len(words)):\n",
        "    words[i] = unify_chartype(words[i])\n",
        "  words = filter_stopwords(words, stopwords)\n",
        "  return words\n",
        "\n",
        "# 文字種を統一する\n",
        "def unify_chartype(text):\n",
        "  text = mojimoji.zen_to_han(text, kana=False, digit=True, ascii=True) # 全角英数=>半角英数\n",
        "  text = mojimoji.han_to_zen(text, kana=True, digit=False, ascii=False) # 半角かな=>全角かな\n",
        "  text = text.lower() # 大文字=>小文字\n",
        "  return text\n",
        "\n",
        "# ストップワードを除去する\n",
        "def filter_stopwords(words, stopwords):\n",
        "  filtered_words = [word for word in words if word not in stopwords]\n",
        "  return filtered_words\n",
        "\n",
        "# ストップワードのリストを返す\n",
        "def get_stopwords():\n",
        "  # SlothLib\n",
        "  slothlib_url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "  http = urllib3.PoolManager()\n",
        "  res = http.request('GET', slothlib_url)\n",
        "  stopwords = res.data.decode('utf-8').split()\n",
        "  return stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whEiJhybgqfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = get_stopwords()\n",
        "\n",
        "words = []\n",
        "for path in train_article_paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  parsed_title = mecab.parse(data[2])\n",
        "  title_words = extract_by_parts(parsed_title, ('名詞'))\n",
        "  words.append(preprocess_words(title_words, stopwords))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkHuyZgcjmMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 辞書, コーパス作成\n",
        "dictionary = gensim.corpora.Dictionary(words)\n",
        "\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.2)\n",
        "\n",
        "dictionary.save_as_text(\"dictionary2.dict.txt\")\n",
        "\n",
        "corpus = [dictionary.doc2bow(w) for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjRFY-jETg0Q",
        "colab_type": "code",
        "outputId": "527fe859-73cc-4b8d-ef47-ffc5cd376765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# LDA\n",
        "topic_N = 10\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=topic_N, id2word=dictionary)\n",
        "\n",
        "for i in range(topic_N):\n",
        "  print('TOPIC:', i, '=>', lda.print_topic(i))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOPIC: 0 => 0.048*\"話題\" + 0.016*\"の\" + 0.016*\"部屋\" + 0.015*\"映画\" + 0.015*\"説教\" + 0.014*\"辛口\" + 0.011*\"虎の巻\" + 0.011*\"xperia\" + 0.010*\"日本\" + 0.010*\"まとめ\"\n",
            "TOPIC: 1 => 0.040*\"映画\" + 0.032*\"オトナ女子\" + 0.024*\"独女\" + 0.016*\"さ\" + 0.013*\"虎の巻\" + 0.011*\"体験\" + 0.011*\"リアル\" + 0.010*\"物\" + 0.010*\"アップ\" + 0.009*\"レア\"\n",
            "TOPIC: 2 => 0.023*\"sports\" + 0.023*\"watch\" + 0.016*\"deji\" + 0.014*\"d\" + 0.013*\"スマホ\" + 0.012*\"提供開始\" + 0.011*\"sc\" + 0.011*\"更新\" + 0.010*\"ドコモ\" + 0.010*\"ソフトウェア\"\n",
            "TOPIC: 3 => 0.059*\"sports\" + 0.059*\"watch\" + 0.025*\"プレゼント\" + 0.022*\"終了\" + 0.018*\"インタビュー\" + 0.015*\"週間ランキング\" + 0.010*\"話題\" + 0.010*\"登場\" + 0.009*\"クリスマス\" + 0.008*\"d\"\n",
            "TOPIC: 4 => 0.038*\"チェック\" + 0.028*\"売れ筋\" + 0.022*\"写真\" + 0.019*\"iphone\" + 0.016*\"発売\" + 0.014*\"0\" + 0.012*\"登場\" + 0.012*\"発表\" + 0.011*\"レポート\" + 0.010*\"nttドコモ\"\n",
            "TOPIC: 5 => 0.030*\"watch\" + 0.030*\"sports\" + 0.021*\"google\" + 0.018*\"話題\" + 0.012*\"理由\" + 0.012*\"ロゴ\" + 0.012*\"iphone\" + 0.009*\"意外\" + 0.008*\"搭載\" + 0.008*\"スポーツ\"\n",
            "TOPIC: 6 => 0.025*\"の\" + 0.024*\"特集\" + 0.022*\"公開\" + 0.020*\"ゴルフ\" + 0.019*\"声\" + 0.019*\"by\" + 0.018*\"cafe\" + 0.015*\"presented\" + 0.014*\"結婚\" + 0.014*\"androidアプリ\"\n",
            "TOPIC: 7 => 0.024*\"対応\" + 0.018*\"d\" + 0.018*\"nttドコモ\" + 0.018*\"xi\" + 0.017*\"搭載\" + 0.016*\"決定\" + 0.016*\"スマホ\" + 0.013*\"発表\" + 0.011*\"登場\" + 0.010*\"活用\"\n",
            "TOPIC: 8 => 0.020*\"smartphone\" + 0.018*\"話題\" + 0.015*\"非難\" + 0.014*\"殺到\" + 0.013*\"批判\" + 0.012*\"独女\" + 0.011*\"声\" + 0.011*\"発言\" + 0.010*\"選手\" + 0.009*\"特集\"\n",
            "TOPIC: 9 => 0.023*\"話題\" + 0.020*\"女子\" + 0.019*\"日本\" + 0.013*\"韓国\" + 0.010*\"s\" + 0.010*\"作品\" + 0.009*\"チェック\" + 0.008*\"ネット\" + 0.008*\"氏\" + 0.008*\"採用\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0UN5ovP9yED",
        "colab_type": "text"
      },
      "source": [
        "## 機械学習"
      ]
    }
  ]
}