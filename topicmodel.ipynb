{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topicmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPMKezgo4p2OqHgi2AmdsAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fukkatsuso/livedoornews-topicmodel/blob/master/topicmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-FlpHjkGkp",
        "colab_type": "text"
      },
      "source": [
        "# トピックモデル\n",
        "\n",
        "## Goal\n",
        "- ライブドアコーパスでWeb記事分類器を作る\n",
        "\n",
        "## Step\n",
        "1. ライブドアコーパスをスクレイピング\n",
        "1. データの前処理\n",
        "  1. 形態素解析 => **MeCab** (+NEologd)\n",
        "  1. 不要語の削除, 語の統一(ステミング)\n",
        "1. トピックモデルの構築 => **gensim**\n",
        "1. 機械学習 => **sklearn**\n",
        "\n",
        "## 参考\n",
        "- [LDAによるトピックモデル with gensim ~ Qiitaのタグからユーザーの嗜好を考える ~](https://qiita.com/shizuma/items/44c016812552ba8a8b88)\n",
        "- [トピックモデルをザックリと理解してサクッと試した](https://qiita.com/d-ogawa/items/c423cd4b01c6ed84a5e7)\n",
        "- [WordCloudとpyLDAvisによるLDAの可視化について](http://www.ie110704.net/2018/12/29/wordcloud%E3%81%A8pyldavis%E3%81%AB%E3%82%88%E3%82%8Blda%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/)\n",
        "- [自然言語処理による文書分類の基礎の基礎、トピックモデルを学ぶ](https://qiita.com/icoxfog417/items/7c944cb29dd7cdf5e2b1)\n",
        "- [scikit-learnとgensimでニュース記事を分類する](https://qiita.com/yasunori/items/31a23eb259482e4824e2)\n",
        "- [文書分類で自然言語処理に触れる](https://colab.research.google.com/drive/1IMjc-RTesapfNCEh0TPmg_ce_qAcV95b#scrollTo=9a9CUjgUXgB6)\n",
        "- [自然言語処理における前処理の種類とその威力](https://qiita.com/Hironsan/items/2466fe0f344115aff177)\n",
        "- [Python3×日本語：自然言語処理の前処理まとめ](https://qiita.com/chamao/items/7edaba62b120a660657e)\n",
        "- [ニュース記事の分類を機械学習で予測する](https://qiita.com/hyo_07/items/ba3d53868b2f55ed9941)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BalvotaRLXB0",
        "colab_type": "text"
      },
      "source": [
        "## データ収集\n",
        "### 対象\n",
        "- [livedoorニュースコーパス](https://www.rondhuit.com/download.html#ldcc)\n",
        "  - [トピックニュース](http://news.livedoor.com/category/vender/news/)\n",
        "  - [Sports Watch](http://news.livedoor.com/category/vender/208/)\n",
        "  - [ITライフハック](http://news.livedoor.com/category/vender/223/)\n",
        "  - [家電チャンネル](http://news.livedoor.com/category/vender/kadench/)\n",
        "  - [MOVIE ENTER](http://news.livedoor.com/category/vender/movie_enter/)\n",
        "  - [独女通信](http://news.livedoor.com/category/vender/90/)\n",
        "  - [エスマックス](http://news.livedoor.com/category/vender/smax/)\n",
        "  - [livedoor HOMME](http://news.livedoor.com/category/vender/homme/)\n",
        "  - [Peachy](http://news.livedoor.com/category/vender/ldgirls/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ0Fu1bAV2G3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dataset\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!mkdir -p dataset/livedoor && tar xvzf ldcc-20140209.tar.gz -C /content/dataset/livedoor --strip-components 1\n",
        "!rm ldcc-20140209.tar.gz\n",
        "\n",
        "# Install MeCab\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "\n",
        "# Install mecab-ipadic-NEologd\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n",
        "\n",
        "!pip install mojimoji"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlMKbg72yh6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 確認\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQF6coxgmfPW",
        "colab_type": "text"
      },
      "source": [
        "## 1.前処理なしLDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhdLIs1SElQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import glob2\n",
        "import MeCab\n",
        "\n",
        "mecab = MeCab.Tagger(\"-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
        "\n",
        "paths = glob2.glob(\"dataset/livedoor/sports-watch/*-*.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPAsBvLueQiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "for path in paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  title = data[2]\n",
        "  words.append(mecab.parse(title).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMnvyMC5eaKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 辞書, コーパス作成\n",
        "dictionary = gensim.corpora.Dictionary(words)\n",
        "\n",
        "dictionary.save_as_text(\"dictionary1.dict.txt\")\n",
        "\n",
        "corpus = [dictionary.doc2bow(w) for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goc0abc8LrSV",
        "colab_type": "code",
        "outputId": "3a515e49-98a7-49c0-83de-534ebb7dfd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# LDA\n",
        "topic_N = 10\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=topic_N, id2word=dictionary)\n",
        "\n",
        "for i in range(topic_N):\n",
        "  print('TOPIC:', i, '=>', lda.print_topic(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 900 documents, updating model once every 900 documents, evaluating perplexity every 900 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "-11.454 per-word bound, 2805.7 perplexity estimate based on a held-out corpus of 900 documents with 14123 words\n",
            "PROGRESS: pass 0, at document #900/900\n",
            "topic #0 (0.100): 0.053*\"の\" + 0.039*\"】\" + 0.039*\"に\" + 0.039*\"【\" + 0.038*\"Watch\" + 0.037*\"Sports\" + 0.033*\"「\" + 0.032*\"」\" + 0.030*\"、\" + 0.023*\"が\"\n",
            "topic #4 (0.100): 0.036*\"「\" + 0.036*\"」\" + 0.032*\"に\" + 0.031*\"が\" + 0.024*\"は\" + 0.021*\"の\" + 0.017*\"・\" + 0.016*\"Sports\" + 0.016*\"【\" + 0.016*\"Watch\"\n",
            "topic #2 (0.100): 0.036*\"、\" + 0.030*\"Sports\" + 0.030*\"【\" + 0.030*\"Watch\" + 0.029*\"】\" + 0.027*\"「\" + 0.026*\"」\" + 0.022*\"に\" + 0.019*\"を\" + 0.014*\"た\"\n",
            "topic #8 (0.100): 0.054*\"の\" + 0.026*\"に\" + 0.025*\"、\" + 0.020*\"は\" + 0.020*\"た\" + 0.018*\"Sports\" + 0.018*\"【\" + 0.017*\"Watch\" + 0.017*\"】\" + 0.017*\"を\"\n",
            "topic #6 (0.100): 0.030*\"・\" + 0.028*\"の\" + 0.028*\"は\" + 0.021*\"Sports\" + 0.021*\"、\" + 0.021*\"【\" + 0.020*\"】\" + 0.020*\"Watch\" + 0.020*\"に\" + 0.012*\"と\"\n",
            "topic diff=7.406228, rho=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TOPIC: 0 => 0.053*\"の\" + 0.039*\"】\" + 0.039*\"に\" + 0.039*\"【\" + 0.038*\"Watch\" + 0.037*\"Sports\" + 0.033*\"「\" + 0.032*\"」\" + 0.030*\"、\" + 0.023*\"が\"\n",
            "TOPIC: 1 => 0.022*\"に\" + 0.009*\"Sports\" + 0.009*\"Watch\" + 0.009*\"【\" + 0.009*\"た\" + 0.009*\"、\" + 0.008*\"】\" + 0.007*\"”\" + 0.007*\"“\" + 0.007*\"ない\"\n",
            "TOPIC: 2 => 0.036*\"、\" + 0.030*\"Sports\" + 0.030*\"【\" + 0.030*\"Watch\" + 0.029*\"】\" + 0.027*\"「\" + 0.026*\"」\" + 0.022*\"に\" + 0.019*\"を\" + 0.014*\"た\"\n",
            "TOPIC: 3 => 0.035*\"の\" + 0.027*\"」\" + 0.026*\"「\" + 0.025*\"は\" + 0.025*\"Sports\" + 0.024*\"【\" + 0.024*\"Watch\" + 0.024*\"】\" + 0.021*\"、\" + 0.021*\"た\"\n",
            "TOPIC: 4 => 0.036*\"「\" + 0.036*\"」\" + 0.032*\"に\" + 0.031*\"が\" + 0.024*\"は\" + 0.021*\"の\" + 0.017*\"・\" + 0.016*\"Sports\" + 0.016*\"【\" + 0.016*\"Watch\"\n",
            "TOPIC: 5 => 0.040*\"】\" + 0.040*\"、\" + 0.039*\"Sports\" + 0.039*\"Watch\" + 0.039*\"【\" + 0.038*\"の\" + 0.023*\"は\" + 0.022*\"に\" + 0.020*\"「\" + 0.020*\"」\"\n",
            "TOPIC: 6 => 0.030*\"・\" + 0.028*\"の\" + 0.028*\"は\" + 0.021*\"Sports\" + 0.021*\"、\" + 0.021*\"【\" + 0.020*\"】\" + 0.020*\"Watch\" + 0.020*\"に\" + 0.012*\"と\"\n",
            "TOPIC: 7 => 0.047*\"、\" + 0.039*\"Sports\" + 0.039*\"】\" + 0.038*\"Watch\" + 0.038*\"【\" + 0.034*\"の\" + 0.030*\"は\" + 0.018*\"「\" + 0.018*\"」\" + 0.016*\"に\"\n",
            "TOPIC: 8 => 0.054*\"の\" + 0.026*\"に\" + 0.025*\"、\" + 0.020*\"は\" + 0.020*\"た\" + 0.018*\"Sports\" + 0.018*\"【\" + 0.017*\"Watch\" + 0.017*\"】\" + 0.017*\"を\"\n",
            "TOPIC: 9 => 0.046*\"の\" + 0.040*\"に\" + 0.036*\"、\" + 0.029*\"」\" + 0.029*\"「\" + 0.023*\"が\" + 0.018*\"は\" + 0.016*\"“\" + 0.016*\"”\" + 0.015*\"Watch\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfuiKA1Zdzsu",
        "colab_type": "text"
      },
      "source": [
        "## 2.前処理ありLDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsVSdhHrlXPe",
        "colab_type": "text"
      },
      "source": [
        "1. 正規化\n",
        "  - 半角かな => 全角かな\n",
        "  - 全角英数 => 半角英数\n",
        "  - 大文字 => 小文字\n",
        "  - 辞書による統一?\n",
        "1. 品詞で取捨選択\n",
        "1. ストップワード除去\n",
        "  - 辞書\n",
        "    - [SlothLib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXl8SSGsfy1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import glob2\n",
        "import mojimoji\n",
        "import nltk\n",
        "import MeCab\n",
        "import urllib3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "mecab = MeCab.Tagger(\"mecabrc -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
        "\n",
        "paths = glob2.glob(\"dataset/livedoor/*/*-*.txt\")\n",
        "\n",
        "# 学習用と評価用に分ける\n",
        "train_rate = 0.7\n",
        "random_state = 0\n",
        "train_article_paths, test_article_paths = train_test_split(paths, train_size=train_rate, random_state=random_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X7IZ8VuqxCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# カテゴリごとの記事数の分散\n",
        "def deviation(random_state, article_paths):\n",
        "  cat = {}\n",
        "  for path in article_paths:\n",
        "    c = path.split('/')[2]\n",
        "    if cat.get(c) == None:\n",
        "      cat[c] = 0\n",
        "    else:\n",
        "      cat[c] += 1\n",
        "\n",
        "  avr = 0\n",
        "  for c in cat:\n",
        "    avr += cat[c]\n",
        "  avr /= len(cat)\n",
        "\n",
        "  d = 0\n",
        "  for c in cat:\n",
        "    d += (cat[c]-avr)**2\n",
        "\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWZB9OOuk7LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ストップワードのリスト\n",
        "def get_stopwords():\n",
        "  # SlothLib\n",
        "  slothlib_url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "  http = urllib3.PoolManager()\n",
        "  res = http.request('GET', slothlib_url)\n",
        "  stopwords = res.data.decode('utf-8').split()\n",
        "  # nltk\n",
        "  stopwords.extend(nltk.corpus.stopwords.words(\"english\"))\n",
        "  # 自分で設定\n",
        "  swlist = ['さ', 'の']\n",
        "  stopwords.extend(swlist)\n",
        "  return stopwords\n",
        "\n",
        "stopwords = get_stopwords()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YEqeIS-heR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 単語のリストから品詞を限定して取り出す\n",
        "def extract_by_parts(parsed, parts):\n",
        "  words = []\n",
        "  lines = parsed.split('\\n')\n",
        "  for line in lines:\n",
        "    feature = line.split('\\t')\n",
        "    if len(feature) == 2:\n",
        "      info = feature[1].split(',')\n",
        "      if info[0] in parts:\n",
        "        if info[6] == '*': \n",
        "          words.append(feature[0])  # 活用なしの語\n",
        "        else:  \n",
        "          words.append(info[6]) # 表記ゆれの対処\n",
        "  return words\n",
        "\n",
        "# 各単語を前処理にかける\n",
        "def preprocess_words(words):\n",
        "  for i in range(len(words)):\n",
        "    words[i] = unify_chartype(words[i])\n",
        "  words = filter_stopwords(words)\n",
        "  return words\n",
        "\n",
        "# 文字種を統一する\n",
        "def unify_chartype(text):\n",
        "  text = mojimoji.zen_to_han(text, kana=False, digit=True, ascii=True) # 全角英数=>半角英数\n",
        "  text = mojimoji.han_to_zen(text, kana=True, digit=False, ascii=False) # 半角かな=>全角かな\n",
        "  text = text.lower() # 大文字=>小文字\n",
        "  return text\n",
        "\n",
        "# ストップワードを除去する\n",
        "def filter_stopwords(words):\n",
        "  filtered_words = [word for word in words if word not in stopwords]\n",
        "  return filtered_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whEiJhybgqfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テキストを分解して品詞で絞り込み，各単語を前処理したリストを返す\n",
        "def text2words(text, parts):\n",
        "  parsed_text = mecab.parse(text)\n",
        "  words = extract_by_parts(parsed_text, parts)\n",
        "  words = preprocess_words(words)\n",
        "  return words\n",
        "\n",
        "# 記事タイトルから学習に必要な単語を取り出す\n",
        "train_words = []\n",
        "for path in train_article_paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  words = text2words(data[2], ('名詞'))\n",
        "  train_words.append(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkHuyZgcjmMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words2corpus(dictionary, words):\n",
        "  return [dictionary.doc2bow(w) for w in words]\n",
        "\n",
        "# 辞書作成\n",
        "dictionary = gensim.corpora.Dictionary(train_words)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.075)\n",
        "dictionary.save_as_text(\"dictionary2.dict.txt\")\n",
        "\n",
        "# コーパス作成\n",
        "train_corpus = words2corpus(dictionary, train_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjRFY-jETg0Q",
        "colab_type": "code",
        "outputId": "c0902fa4-f95f-46f7-c159-1d49d219fea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# LDA\n",
        "topic_N = 10\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=train_corpus, num_topics=topic_N, id2word=dictionary)\n",
        "\n",
        "for i in range(topic_N):\n",
        "  print('TOPIC:', i, '=>', lda.print_topic(i))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOPIC: 0 => 0.020*\"声\" + 0.018*\"結婚\" + 0.016*\"発表\" + 0.015*\"映画\" + 0.015*\"女性\" + 0.012*\"iphone\" + 0.012*\"オトナ女子\" + 0.012*\"虎の巻\" + 0.011*\"術\" + 0.010*\"映画批評\"\n",
            "TOPIC: 1 => 0.028*\"映画\" + 0.019*\"韓国\" + 0.018*\"提供開始\" + 0.016*\"2012年\" + 0.015*\"更新\" + 0.015*\"ソフトウェア\" + 0.014*\"声\" + 0.014*\"編\" + 0.013*\"決定\" + 0.013*\"au\"\n",
            "TOPIC: 2 => 0.020*\"終了\" + 0.016*\"google\" + 0.015*\"批判\" + 0.014*\"ロゴ\" + 0.014*\"プレゼント\" + 0.012*\"ゲーム\" + 0.011*\"twitter\" + 0.011*\"ゴルフ\" + 0.010*\"殺到\" + 0.010*\"映画\"\n",
            "TOPIC: 3 => 0.027*\"写真\" + 0.022*\"日本\" + 0.020*\"iphone\" + 0.011*\"韓国\" + 0.010*\"発言\" + 0.010*\"flash back\" + 0.010*\"挑戦\" + 0.009*\"twitter\" + 0.009*\"電子書籍\" + 0.009*\"独女\"\n",
            "TOPIC: 4 => 0.031*\"チェック\" + 0.026*\"登場\" + 0.019*\"iphone\" + 0.019*\"売れ筋\" + 0.018*\"アプリ\" + 0.016*\"ニュース\" + 0.015*\"発売\" + 0.014*\"smartphone\" + 0.012*\"搭載\" + 0.012*\"対応\"\n",
            "TOPIC: 5 => 0.017*\"watch\" + 0.017*\"sports\" + 0.016*\"dvd\" + 0.015*\"虎の巻\" + 0.013*\"特集\" + 0.012*\"インタビュー\" + 0.012*\"仕事\" + 0.012*\"最強\" + 0.011*\"新生活\" + 0.010*\"モテ\"\n",
            "TOPIC: 6 => 0.074*\"watch\" + 0.074*\"sports\" + 0.020*\"氏\" + 0.016*\"チェック\" + 0.016*\"年収\" + 0.014*\"インタビュー\" + 0.014*\"映画\" + 0.013*\"サービス\" + 0.012*\"売れ筋\" + 0.011*\"1000万円\"\n",
            "TOPIC: 7 => 0.025*\"対応\" + 0.021*\"nttドコモ\" + 0.021*\"スマホ\" + 0.018*\"開始\" + 0.016*\"搭載\" + 0.015*\"発表\" + 0.012*\"android icecream sandwich\" + 0.012*\"独女\" + 0.011*\"ics\" + 0.010*\"向け\"\n",
            "TOPIC: 8 => 0.031*\"watch\" + 0.031*\"sports\" + 0.016*\"説教\" + 0.016*\"辛口\" + 0.015*\"部屋\" + 0.015*\"vol.4\" + 0.014*\"xperia\" + 0.013*\"恋\" + 0.012*\"公開\" + 0.011*\"激怒\"\n",
            "TOPIC: 9 => 0.021*\"女子\" + 0.015*\"恋愛\" + 0.014*\"cafe\" + 0.013*\"presented\" + 0.012*\"チェック\" + 0.010*\"deji\" + 0.010*\"美女\" + 0.010*\"週間ランキング\" + 0.010*\"2012\" + 0.010*\"次世代\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM_JfOnyphfZ",
        "colab_type": "text"
      },
      "source": [
        "## テスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QBE3FmCpmjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テスト用データとして，記事タイトルから単語を取り出す\n",
        "test_words = []\n",
        "for path in test_article_paths:\n",
        "  data = open(path, 'r', encoding=\"utf-8\").read().split('\\n')\n",
        "  words = text2words(data[2], ('名詞'))\n",
        "  test_words.append(words)\n",
        "\n",
        "test_corpus = words2corpus(dictionary, test_words)\n",
        "\n",
        "# for i in range(len(test_article_paths)):\n",
        "#   category = test_article_paths[i].split('/')[2]\n",
        "#   title = open(test_article_paths[i], 'r', encoding=\"utf-8\").read().split('\\n')[2]\n",
        "#   print(category, '\\n', title, '\\n', lda[test_corpus[i]], '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhB-fWKnoWMo",
        "colab_type": "text"
      },
      "source": [
        "## 機械学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaweF0MmoaWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4b741588-4676-4ebb-ec9a-86c8dc52149b"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# カテゴリ\n",
        "category = {\n",
        "  'dokujo-tsushin': 0,\n",
        "  'it-life-hack': 1,\n",
        "  'kaden-channel': 2,\n",
        "  'livedoor-homme': 3,\n",
        "  'movie-enter': 4,\n",
        "  'peachy': 5,\n",
        "  'smax': 6,\n",
        "  'sports-watch': 7,\n",
        "  'topic-news': 8\n",
        "}\n",
        "\n",
        "def data_and_labels(paths, corpus, lda, topic_N):\n",
        "  data = []\n",
        "  labels = []\n",
        "  for i in range(len(paths)):\n",
        "    # data\n",
        "    w = [0] * topic_N\n",
        "    for topic in lda[corpus[i]]:\n",
        "      w[topic[0]] = topic[1]\n",
        "    data.append(w)\n",
        "    # label\n",
        "    label = category[paths[i].split('/')[2]]\n",
        "    labels.append(label)\n",
        "  return data, labels\n",
        "\n",
        "# トピックモデル構築に使ったデータ\n",
        "train_data, train_label = data_and_labels(train_article_paths, train_corpus, lda, topic_N)\n",
        "\n",
        "# ランダムフォレストで学習\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(train_data, train_label)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR5FZttnm6Ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f4a6943-fa83-4f29-c8db-9dcaa65aa18d"
      },
      "source": [
        "test_data, test_label = data_and_labels(test_article_paths, test_corpus, lda, topic_N)\n",
        "\n",
        "random_forest.score(test_data, test_label)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4292175486205337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}